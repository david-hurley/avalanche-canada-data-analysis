{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "The objective of this notebook is to explore historical avalanche danger ratings as reported by Avalanche Canada (https://www.avalanche.ca/). Specifically, this notebook attempts to assess anomalies between day of danger ratings and one- and two-day out forecasted danger ratings. \n",
    "\n",
    "#### The Data\n",
    "The dataset is created with scrape_export_data.py and cleaned with clean_scraped_data.py. The dataset covers the period 2011-11-02 through 2020-04-15 and includes day of danger ratings and one- and two-day out forecasted danger ratings. Additionally, any snowpack problems, as written by the forecaster, are included for day of danger ratings. The dataset was scraped from the Avalanche Canada forecast archives (https://www.avalanche.ca/forecasts/archives) using Selenium and Python. \n",
    "\n",
    "#### Key Terms\n",
    "A few key terms that are important to understand in this analysis:\n",
    "* Day of Conditions - The current days avalanche danger ratings. This is the most up to date condition forecast.\n",
    "* Day of Conditions Plus 1 and 2 - Tomorrow and the day after tomorrows forecasted danger ratings. These are always superceded by the Day of Conditions.\n",
    "* Avalanche Status Code - Avalanche danger ratings on a scale of 1-5 with 5 being Extreme and 1 being Low\n",
    "* Alpine - Elevations above treeline\n",
    "* Treeline - Elevations at or near treeline\n",
    "* Below Treeline - Elevations in the trees\n",
    "* Forecast Anamoly - Difference between forecasted value and observed/reported value.\n",
    "\n",
    "#### Notebook Outline\n",
    "A basic outline for the notebook is as follows:\n",
    "1. Load data\n",
    "2. Clean data\n",
    "3. Explore Data - visualize danger rating data\n",
    "4. Data Analysis - assess forecasted danger rating anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def percent_missing(df, name):\n",
    "    '''function to compute the percent NaN data in each column of a dataframe'''\n",
    "    \n",
    "    perc_missing = df.isna().sum() / len(df) * 100\n",
    "    \n",
    "    return print('Percent Missing in Each Column Before Filtering:', name, '\\n \\n', perc_missing, '\\n')\n",
    "\n",
    "def zero_matrix():\n",
    "    '''function to create a zero matrix for seaborn heatmap plotting'''\n",
    "    \n",
    "    df = pd.DataFrame(np.zeros((5,5)), index=[5,4,3,2,1])  # reverse for plotting purposes\n",
    "    df.columns = [1,2,3,4,5]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define absolute path to raw data folder\n",
    "raw_data_root_path = os.path.abspath('../data/raw')\n",
    "\n",
    "# raw data filenames\n",
    "current_conditions_file = 'current_avalanche_conditions_sea_to_sky_RAW.csv'  # day of forecast\n",
    "current_plus1_conditions_file = 'current_plus1_avalanche_conditions_sea_to_sky_RAW.csv'  # tomorrow forecast\n",
    "current_plus2_conditions_file = 'current_plus2_avalanche_conditions_sea_to_sky_RAW.csv'  # day after tomorrow forecast\n",
    "  \n",
    "# define absolute path to raw data files\n",
    "current_conditions_path = os.path.join(raw_data_root_path, current_conditions_file)\n",
    "current_plus1_conditions_path = os.path.join(raw_data_root_path, current_plus1_conditions_file)\n",
    "current_plus2_conditions_path = os.path.join(raw_data_root_path, current_plus2_conditions_file) \n",
    "\n",
    "# load raw data to dataframe, parse dates as datetime index\n",
    "df_raw_current = pd.read_csv(current_conditions_path, parse_dates=['date_valid'])\n",
    "df_raw_current_plus1 = pd.read_csv(current_plus1_conditions_path, parse_dates=['date_valid'])\n",
    "df_raw_current_plus2 = pd.read_csv(current_plus2_conditions_path, parse_dates=['date_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_current.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dataframes to clean\n",
    "dataframes = [df_raw_current, df_raw_current_plus1, df_raw_current_plus2]\n",
    "\n",
    "# check for percent missing values for each dataframe\n",
    "for df, name in zip(dataframes, ['current', 'current+1', 'current+2']):\n",
    "    percent_missing(df, name)\n",
    "    \n",
    "# # replace zero values with NaN and remove any row that has NaN value for avalanche status\n",
    "for df in dataframes:\n",
    "    df.replace(0.0, np.nan, inplace=True)\n",
    "    df.dropna(subset=['alpine_status_code', 'treeline_status_code', 'belowtree_status_code'], inplace=True)\n",
    "    \n",
    "# remove extra row in current day forecast\n",
    "df_raw_current.drop(columns=['Unnamed: 8'], inplace=True)\n",
    "\n",
    "# replace missing problem text with No Text statement\n",
    "df_raw_current['problems'].replace(np.nan, 'No Text', inplace=True)\n",
    "\n",
    "# assign dataframes to new processed dataframe\n",
    "df_cleaned_current = df_raw_current\n",
    "df_cleaned_current_plus1 = df_raw_current_plus1\n",
    "df_cleaned_current_plus2 = df_raw_current_plus2\n",
    "\n",
    "# save cleaned dataset\n",
    "df_cleaned_current.to_csv('../data/cleaned/current_avalanche_danger_ratings_sea_to_sky_2011_2012.csv')\n",
    "df_cleaned_current_plus1.to_csv('../data/cleaned/current_plus1_avalanche_danger_ratings_sea_to_sky_2011_2012.csv')\n",
    "df_cleaned_current_plus2.to_csv('../data/cleaned/current_plus2_avalanche_danger_ratings_sea_to_sky_2011_2012.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_current = pd.read_csv('../data/cleaned/current_avalanche_danger_ratings_sea_to_sky_2011_2012.csv')\n",
    "df_cleaned_current_plus1 = pd.read_csv('../data/cleaned/current_plus1_avalanche_danger_ratings_sea_to_sky_2011_2012.csv')\n",
    "df_cleaned_current_plus2 = pd.read_csv('../data/cleaned/current_plus2_avalanche_danger_ratings_sea_to_sky_2011_2012.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(82/255, 186/255, 74/255), (255/255, 243/255, 0/255), (247/255, 146/255, 24/255), \n",
    "          (239/255, 28/255, 41/255), (0/255, 0/255, 0/255)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of \"Day of Conditions\" in Sea to Sky Region 2011-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent occurence of each avalanche condition for each elevation\n",
    "avy_perc_occur_alpine = df_cleaned_current['alpine_status_code'].value_counts(normalize=True) * 100\n",
    "avy_perc_occur_treeline = df_cleaned_current['treeline_status_code'].value_counts(normalize=True) * 100\n",
    "avy_perc_occur_belowtree = df_cleaned_current['belowtree_status_code'].value_counts(normalize=True) * 100\n",
    "\n",
    "# combine into one dataframe for plotting ease\n",
    "avy_perc_occur_all_elev = pd.concat([avy_perc_occur_alpine, avy_perc_occur_treeline, avy_perc_occur_belowtree], axis=1)\n",
    "\n",
    "# properties for donut charts\n",
    "status_labels = ['Low', 'Moderate', 'Considerable', 'High', 'Extreme']\n",
    "title_labels = ['Alpine', 'Treeline', 'Belowtree']\n",
    "\n",
    "# create donut chart for each elevation\n",
    "fig, ax = plt.subplots(3,1,figsize=(7,12), subplot_kw=dict(aspect=\"equal\"), facecolor='white')\n",
    "\n",
    "for i, col in enumerate(avy_perc_occur_all_elev):\n",
    "    \n",
    "    data = avy_perc_occur_all_elev[col]  # column of dataframe to render\n",
    "    data.dropna(inplace=True)  # remove any NaN rows\n",
    "        \n",
    "    donut_labels = [str(round(y,1)) + '%' for y in list(data)]\n",
    "            \n",
    "    wedges, texts = ax[i].pie(data, wedgeprops=dict(width=0.5), startangle=-40, colors = colors)  # create donut chart\n",
    "        \n",
    "    #  create lines and labels\n",
    "    kw = dict(arrowprops=dict(arrowstyle=\"-\"), zorder=0, va=\"center\")\n",
    "    \n",
    "    for j, p in enumerate(wedges):\n",
    "        ang = (p.theta2 - p.theta1)/2. + p.theta1  # a bit of 9th grade geometry\n",
    "        y = np.sin(np.deg2rad(ang))\n",
    "        x = np.cos(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "        connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "        kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
    "        ax[i].annotate(donut_labels[j], xy=(x, y), xytext=(1.1*np.sign(x), 1.4*y),\n",
    "                    horizontalalignment=horizontalalignment, **kw)\n",
    "        \n",
    "    ax[i].text(0,0,title_labels[i], fontsize='14', ha='center', va='center')  # add title to donut center\n",
    "    \n",
    "    # overall title\n",
    "    if i == 0:\n",
    "        ax[i].text(-2,1.2,'Avalanche Danger Ratings \\n(Sea to Sky Region: 2011-2020)', fontsize='12')\n",
    "    \n",
    "    # legend\n",
    "    if i == 1:\n",
    "        ax[i].legend(wedges, status_labels, title=\"Danger Rating\", loc='center left', \n",
    "                     bbox_to_anchor=(-0.7, 0.25, 0.5, 0.5), prop=dict(size=12))\n",
    "# notes about dataset       \n",
    "ax[2].text(-2, -2, '*No extreme days below treeline from 2011 to 2020 \\n **Extreme days in Alpine/Treeline are rare')\n",
    "\n",
    "plt.savefig('../figures/avalanche-danger-ratings-sea-to-sky-2011-2020.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of \"Day of Conditions\" by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of years in dataset\n",
    "years = df_cleaned_current.date_valid.dt.year.unique()\n",
    "\n",
    "# initialize empty list\n",
    "alpine_conditions = []\n",
    "treeline_conditions = []\n",
    "belowtree_conditions = []\n",
    "year_labels = []\n",
    "\n",
    "for i, year in enumerate(years[:-1]):\n",
    "    \n",
    "    # define avalanche season\n",
    "    start = '{}-11-01'.format(year)\n",
    "    end = '{}-05-01'.format(year+1)\n",
    "        \n",
    "    # create label for figure\n",
    "    label = '{} - {}'.format(year, year+1)\n",
    "    year_labels.append(label)\n",
    "        \n",
    "    # creates list of avalanche conditions for each season at each elevation\n",
    "    alpine_conditions.append(df_cleaned_current[(df_cleaned_current.date_valid > start) & \n",
    "                                                (df_cleaned_current.date_valid < end)]['alpine_status_code'].value_counts(normalize=True) * 100)\n",
    "   \n",
    "    treeline_conditions.append(df_cleaned_current[(df_cleaned_current.date_valid > start) & \n",
    "                                                (df_cleaned_current.date_valid < end)]['treeline_status_code'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    belowtree_conditions.append(df_cleaned_current[(df_cleaned_current.date_valid > start) & \n",
    "                                                (df_cleaned_current.date_valid < end)]['belowtree_status_code'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "# creates dataframes from lists\n",
    "df_alpine = pd.DataFrame(alpine_conditions)\n",
    "df_treeline = pd.DataFrame(treeline_conditions)\n",
    "df_belowtree = pd.DataFrame(belowtree_conditions)\n",
    "                                     \n",
    "fig, ax = plt.subplots(3,1,figsize=(14,10), facecolor=\"white\")\n",
    "\n",
    "# define plot properties\n",
    "title_label = ['Alpine', 'Treeline', 'Belowtree']\n",
    "x = np.arange(len(alpine_conditions))\n",
    "width = 0.15\n",
    "\n",
    "# create bar charts\n",
    "for i, df in enumerate([df_alpine, df_treeline, df_belowtree]):\n",
    "    \n",
    "    rects1 = ax[i].bar(x - width*2, df[1], width, label='Low', zorder=3, color=colors[0])\n",
    "    rects2 = ax[i].bar(x - width, df[2], width, label='Moderate', zorder=3, color=colors[1])\n",
    "    rects3 = ax[i].bar(x, df[3], width, label='Considerable', zorder=3, color=colors[2])\n",
    "    rects4 = ax[i].bar(x + width, df[4], width, label='High', zorder=3, color=colors[3])\n",
    "    \n",
    "    if 5 in df.columns:\n",
    "        rects5 = ax[i].bar(x + width*2, df[5], width, label='Extreme', zorder=3, color=colors[4])\n",
    "        \n",
    "    ax[i].set_xticks(x)\n",
    "    ax[i].set_xticklabels(year_labels)\n",
    "    ax[i].set_title('{} Danger Ratings by Season (Sea to Sky Region)'.format(title_label[i]))\n",
    "    ax[i].set_ylabel('Percentage of Season (%)')\n",
    "    ax[i].grid(zorder=0)\n",
    "\n",
    "# add legend and text\n",
    "ax[0].legend(title=\"Danger Ratings\", loc='center left', bbox_to_anchor=(0, 0.64, 0.5, 0.5), ncol=5)\n",
    "ax[0].text(-0.5,37,'Last moderate \\nstrength La Nina')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "\n",
    "plt.savefig('../figures/avalanche-danger-ratings-by-season-sea-to-sky-2011-2020.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of \"Day of Conditions\" by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of months in dataset\n",
    "months = [11, 12, 1, 2, 3, 4]\n",
    "\n",
    "# initialize empty lists\n",
    "alpine_conditions = []\n",
    "treeline_conditions = []\n",
    "belowtree_conditions = []\n",
    "year_labels = []\n",
    "\n",
    "for month in months:\n",
    "    \n",
    "    # creates list of avalanche danger ratins for each month at each elevation\n",
    "    alpine_conditions.append(df_cleaned_current[df_cleaned_current.date_valid.dt.month == month]\n",
    "                             ['alpine_status_code'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    treeline_conditions.append(df_cleaned_current[df_cleaned_current.date_valid.dt.month == month]\n",
    "                             ['treeline_status_code'].value_counts(normalize=True) * 100)\n",
    "        \n",
    "    belowtree_conditions.append(df_cleaned_current[df_cleaned_current.date_valid.dt.month == month]\n",
    "                             ['belowtree_status_code'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "# creates dataframes from lists\n",
    "df_alpine = pd.DataFrame(alpine_conditions)\n",
    "df_treeline = pd.DataFrame(treeline_conditions)\n",
    "df_belowtree = pd.DataFrame(belowtree_conditions)\n",
    "                            \n",
    "fig, ax = plt.subplots(3,1,figsize=(14,10), facecolor=\"white\")\n",
    "\n",
    "# define plot properties\n",
    "title_label = ['Alpine', 'Treeline', 'Belowtree']\n",
    "month_labels = ['Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr']\n",
    "x = np.arange(len(months))\n",
    "width = 0.15\n",
    "\n",
    "# create bar charts\n",
    "for i, df in enumerate([df_alpine, df_treeline, df_belowtree]):\n",
    "    \n",
    "    rects1 = ax[i].bar(x - width*2, df[1], width, label='Low', zorder=3, color=colors[0])\n",
    "    rects2 = ax[i].bar(x - width, df[2], width, label='Moderate', zorder=3, color=colors[1])\n",
    "    rects3 = ax[i].bar(x, df[3], width, label='Considerable', zorder=3, color=colors[2])\n",
    "    rects4 = ax[i].bar(x + width, df[4], width, label='High', zorder=3, color=colors[3])\n",
    "    \n",
    "    # trick to avoid plotting error when extreme doesn't exist\n",
    "    if 5 in df.columns:\n",
    "        rects5 = ax[i].bar(x + width*2, df[5], width, label='Extreme', zorder=3, color=colors[4])\n",
    "        \n",
    "    ax[i].set_xticks(x)\n",
    "    ax[i].set_xticklabels(month_labels)\n",
    "    ax[i].set_title('{} Danger Ratings by Month (Sea to Sky Region: 2011-2020)'.format(title_label[i]))\n",
    "    ax[i].set_ylabel('Percentage of Month (%)')\n",
    "    ax[i].grid(zorder=0)\n",
    "    \n",
    "# legend and limits\n",
    "ax[0].legend(title=\"Danger Ratings\", loc='center left', bbox_to_anchor=(0.5, 0.6, 0.5, 0.5), ncol=5)\n",
    "ax[0].set_ylim([0, 70])\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "\n",
    "plt.savefig('../figures/avalanche-danger-ratings-by-month-sea-to-sky-2011-2020.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most common avalanche problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of problems to search in text based on Simon Fraser document\n",
    "avy_problem_types = ['storm slab', 'wind slab', 'wet avalanche', 'cornice', 'persistent slab', \n",
    "                     'deep persistent', 'wet loose']\n",
    "\n",
    "# initialize empty list\n",
    "count = []\n",
    "\n",
    "# loops through reported problems and matches with problem types\n",
    "for problem in df_cleaned_current.problems:\n",
    "    \n",
    "    for avy_problem_type in avy_problem_types:\n",
    "        \n",
    "        if avy_problem_type in problem.lower().replace('.',''):\n",
    "            count.append(avy_problem_type)\n",
    "            \n",
    "# creates a count of the most common problem types \n",
    "common_problem_count = pd.DataFrame(Counter(count).most_common()).sort_values(by=[1])\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(14,5), facecolor=\"white\")\n",
    "\n",
    "# bar plot\n",
    "ax.barh(common_problem_count[0], common_problem_count[1], zorder=3)\n",
    "ax.set_xlabel('Number of Times Mentioned')\n",
    "ax.set_title('Most Common Avalanche Problems (Sea to Sky Region: 2011-2020)')\n",
    "ax.text(150,0.1,'*Based on problem text as written by forecasters. \\n This is an estimate and actual counts may differ.')\n",
    "ax.grid(zorder=0)\n",
    "\n",
    "plt.savefig('../figures/frequency-of-avalanche-problem-types-sea-to-sky-2011-2020.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Danger Rating Forecast Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge day of and forecasted datasets\n",
    "df_now_plus1 = pd.merge(df_cleaned_current, df_cleaned_current_plus1, left_on='date_valid', right_on='date_valid')\n",
    "df_now_plus2 = pd.merge(df_cleaned_current, df_cleaned_current_plus2, left_on='date_valid', right_on='date_valid')\n",
    "\n",
    "# initiate empty matrix to fill\n",
    "df_alpine_now_plus1 = zero_matrix()\n",
    "df_treeline_now_plus1 = zero_matrix()\n",
    "df_belowtree_now_plus1 = zero_matrix()\n",
    "\n",
    "df_alpine_now_plus2 = zero_matrix()\n",
    "df_treeline_now_plus2 = zero_matrix()\n",
    "df_belowtree_now_plus2 = zero_matrix()\n",
    "\n",
    "# for each danger rating in day of conditions check frequency of forecasted danger rating across all ratings\n",
    "for i in [1,2,3,4,5]:\n",
    "    \n",
    "    for j in [1,2,3,4,5]:\n",
    "        \n",
    "        alp_tmp_1 = len(df_now_plus1[(df_now_plus1.alpine_status_code_x == j) & (df_now_plus1.alpine_status_code_y == i)])\n",
    "        alp_tmp_1 = (alp_tmp_1 / len(df_now_plus1[df_now_plus1.alpine_status_code_y == i])) * 100\n",
    "        \n",
    "        alp_tmp_2 = len(df_now_plus2[(df_now_plus2.alpine_status_code_x == j) & (df_now_plus2.alpine_status_code_y == i)])\n",
    "        if i == 5:\n",
    "            alp_tmp_2 = 0\n",
    "        else:\n",
    "            alp_tmp_2 = (alp_tmp_2 / len(df_now_plus2[df_now_plus2.alpine_status_code_y == i])) * 100\n",
    "        \n",
    "        treeline_tmp_1 = len(df_now_plus1[(df_now_plus1.treeline_status_code_x == j) & (df_now_plus1.treeline_status_code_y == i)])\n",
    "        treeline_tmp_1 = (treeline_tmp_1 / len(df_now_plus1[df_now_plus1.treeline_status_code_y == i])) * 100\n",
    "        \n",
    "        treeline_tmp_2 = len(df_now_plus2[(df_now_plus2.treeline_status_code_x == j) & (df_now_plus2.treeline_status_code_y == i)])\n",
    "        if i == 5:\n",
    "            treeline_tmp_2 = 0\n",
    "        else:\n",
    "            treeline_tmp_2 = (treeline_tmp_2 / len(df_now_plus2[df_now_plus2.treeline_status_code_y == i])) * 100\n",
    "        \n",
    "        belowtree_tmp_1 = len(df_now_plus1[(df_now_plus1.belowtree_status_code_x == j) & (df_now_plus1.belowtree_status_code_y == i)])\n",
    "        belowtree_tmp_2 = len(df_now_plus2[(df_now_plus2.belowtree_status_code_x == j) & (df_now_plus2.belowtree_status_code_y == i)])\n",
    "\n",
    "        if i == 5:\n",
    "            belowtree_tmp_1 = 0\n",
    "            belowtree_tmp_2 = 0\n",
    "        else:\n",
    "            belowtree_tmp_1 = (belowtree_tmp_1 / len(df_now_plus1[df_now_plus1.belowtree_status_code_y == i])) * 100\n",
    "            belowtree_tmp_2 = (belowtree_tmp_2 / len(df_now_plus2[df_now_plus2.belowtree_status_code_y == i])) * 100\n",
    "\n",
    "        # store data in prealocated dataframe\n",
    "        df_alpine_now_plus1[i].loc[j] = alp_tmp_1\n",
    "        df_treeline_now_plus1[i].loc[j] = treeline_tmp_1\n",
    "        df_belowtree_now_plus1[i].loc[j] = belowtree_tmp_1\n",
    "        \n",
    "        df_alpine_now_plus2[i].loc[j] = alp_tmp_2\n",
    "        df_treeline_now_plus2[i].loc[j] = treeline_tmp_2\n",
    "        df_belowtree_now_plus2[i].loc[j] = belowtree_tmp_2\n",
    "        \n",
    "# create heatmaps\n",
    "df_list = [df_alpine_now_plus1, df_alpine_now_plus2, df_treeline_now_plus1, \n",
    "           df_treeline_now_plus2, df_belowtree_now_plus1, df_belowtree_now_plus2]\n",
    "\n",
    "# labels \n",
    "labels = ['Alpine', '', 'Treeline', '', 'Belowtree', '']\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10,14), facecolor=\"white\")\n",
    "ax = ax.flatten()\n",
    "\n",
    "# create heatmaps\n",
    "for i, df in enumerate(df_list):\n",
    "\n",
    "    ax[i] = sns.heatmap(df, annot=df, fmt='.3g', cmap='mako_r', cbar=False, square=True, ax=ax[i])\n",
    "    \n",
    "    ax[i].set_xlabel('Forecasted Danger Rating')\n",
    "    for t in ax[i].texts: \n",
    "        t.set_text(t.get_text() + \"%\")\n",
    "        \n",
    "    if i % 2 == 0:\n",
    "        ax[i].text(-1.3,3,labels[i], rotation=90, fontweight='bold', fontsize='12')\n",
    "        ax[i].set_ylabel('Reported Danger Rating')\n",
    "\n",
    "ax[0].set_title('One Day Out Forecast Anomaly', fontweight='bold', fontsize='12')\n",
    "ax[1].set_title('Two Day Out Forecast Anomaly', fontweight='bold', fontsize='12')\n",
    "\n",
    "ax[4].text(0, 7.5, 'Percentage of time a forecasted danger rating aligned or differed \\\n",
    "from a reported danger \\nrating. For example, when a danger rating of 4 in the alpine was forecasted one day out \\\n",
    "\\nthen 72.1% of the time the reported danger rating was 4.', fontsize='12')\n",
    "\n",
    "plt.savefig('../figures/one_and_two_day_forecast_anomaly.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
